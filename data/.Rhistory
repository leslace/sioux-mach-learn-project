length(winner)
length(opponent)
winner[6:10]
winner[3, 5, 10:12]
winner[c(3, 5, 10:1)]
replace(winners, [6, 8],  189)
replace(winners, 6,  189)
replace(winner, 6,  189)
replace(winner, 7:8,  189)
mean(winner)
mean(opponent)
mean(opponent)
mean(winner) - mean(opponent)
#calculate the variance
var(winner)
#calculate the standard deviation
sd(winner)
winner_var <- sum((winner - mean(winner))^2) / (length(winner)-1)
#manual standard deviation
winner_sd <- sqrt(winner_var)
winner_var
winner_sd
grades <- c(4.2, 2.3, 5.6, 4.5, 4.8, 3.9, 5.9, 2.4, 5.9, 6, 4, 3.7, 5, 5.2, 4.5, 3.6, 5, 6, 2.8, 3.3, 5.5, 4.2, 4.9, 5.1)
sort(grades)
mean(grades)
median(grades)
sort(grades)
grades_2 <- sort(grades_1)
mean(grades_1)
grades_1 <- c(4.2, 2.3, 5.6, 4.5, 4.8, 3.9, 5.9, 2.4, 5.9, 6, 4, 3.7, 5, 5.2, 4.5, 3.6, 5, 6, 2.8, 3.3, 5.5, 4.2, 4.9, 5.1)
2.8, 3.3, 5.5, 4.2, 4.9, 5.1)
grades_1 <- c(4.2, 2.3, 5.6, 4.5, 4.8, 3.9, 5.9, 2.4, 5.9, 6, 4, 3.7, 5, 5.2, 4.5, 3.6, 5, 6, 2.8, 3.3, 5.5, 4.2, 4.9, 5.1)
grades_2 <- sort(grades_1)
grades_2[c(9,10,11)] <- 1
median(grades_2)
median(grades_1)
mean(grades_2)
mean(grades_1)
boxplot(grades_1)
par(mar=c(0,2,2,0))
boxplot(grades_1,
grades_2,
main = "Boxplot",
col = c("orange", "lightblue")
)
setwd("~/Downloads")
#load csv file
married <- read.csv("husband_wife.csv")
("husband_wife.csv")
#load csv file
married <- read.csv("husband_wife.csv")
setwd("~/switchdrive/SA01_Exam")
knitr::opts_chunk$set(echo = TRUE)
#load csv file
married <- read.csv("husband_wife.csv")
summary(married)
boxplot(married,
main = "Boxplot",
col = c("orange", "lightblue")
)
age_man <- hw[, 1]
#selecting the variables needed
age_man <- married[, 1]
age_woman <- married[, 3]
for (i in -12:20){
lines(c(-2,2),c(i,i),col="gray95",lty="dashed")
}
for (i in -12:20) {
lines (c(-2,2),c(i,i),col="gray95",lty="dashed")
}
boxplot
#selecting the variables needed
age_man <- married[, 1]
age_woman <- married[, 3]
boxplot <- boxplot(age_man-age_woman, col="orange")
for (i in -12:20) {
lines (c(-2,2),c(i,i),col="gray95",lty="dashed")
}
boxplot
data("anscombe")
reg <- lm(anscombe$y1 ~ anscombe$x1)
reg2 <- lm(anscombe$y2 ~ anscombe$x2)
reg3 <- lm(anscombe$y3 ~ anscombe$x3)
reg4 <- lm(anscombe$y4 ~ anscombe$x4)
iq <- rnorm(n = 200, mean = 100, sd = 15)
hist(iq,
col = "darkseagreen3",
xlab = "Points in the IQ test",
ylab = "Number of people",
main = "Distribution of points in an IQ test"
)
#choosing the right classes
hist(iq,
breaks = "sturges", # default R
xlab = "Sturges rule from R",
col = "darkseagreen3"
)
geyser <- faithful
head(geyser)
rect(80.02, 0, 80.04, 23.1, col="darkseagreen4")
hist(scaleA,
freq = F,
main = "Histogram of balance A",
col = "darkseagreen3",
ylim = c(0, 25)
)
rect(80.02, 0, 80.04, 23.1, col="darkseagreen4")
hist(geyser[, "waiting"], breaks = seq(41, 96, by = 11))
par(mfrow = c(2,2))
hist(geyser[, "waiting"])
hist(geyser[, "waiting"], breaks = 20)
hist(geyser[, "waiting"], breaks = seq(41, 96, by = 11))
hist(geyser[, "eruptions"])
par(mfrow = c(2,2))
hist(geyser[, "waiting"])
hist(geyser[, "waiting"], breaks = 20)
hist(geyser[, "waiting"], breaks = seq(41, 96, by = 11))
hist(geyser[, "eruptions"])
Example Problem 4.5
x <- c(-5, -4, 1, 3, 6)
p <- c(0.3, 0.1, 0.1, 0.2, 0.3)
sum(x*p)
x <- 2:12
p <- c(1, 2, 3, 4, 5, 6, 5, 4, 3, 2, 1) / 36
E
x <- 2:12
p <- c(1, 2, 3, 4, 5, 6, 5, 4, 3, 2, 1) / 36
E <- sum(x*p)
E
#calculate the standard diviation
E <- sum(x*p)
var_X <- sum((x-E)^2*p)
var_X
sigma
sigma <- sqrt(var_X)
sigma
1 - pnorm(q = 1.85, mean = 1.8, sd = 0.074)
pnorm(q = 1.8, mean = 1.8, sd = 0.074) - pnorm(1.7, 1.8, 0.074)
qnorm(p = c(0.25, 0.75), mean = 1.8, sd = 0.074)
qnorm(p = 0.95, mean = 1.8, sd = 0.074)
pnorm(q = 2.5, mean = 4, sd = 1.25)
1 - pnorm(q = 5, mean = 4, sd = 1.25)
knitr::opts_chunk$set(echo = TRUE, messages = FALSE)
packages <- c("dplyr","readxl", "curl", "ggplot2", "ggrepel", "maps", "plotly", "stringr", "tm", "wordcloud2", "tidyverse", "RColorBrewer", "ggwordcloud", "viridis", "bookdown", "utils", "leaps", "broom","GGally", "e1071", "caret", "mgcv")
package.check <- lapply(packages, FUN = function(x) {
if (!require(x, character.only = TRUE)) {
install.packages(x, dependencies = TRUE)
library(x, character.only = TRUE)
}
})
svm_linear_grid
#we use the predict function and pass the trained model
#and as new data we pass the testing data set
test_pred <- predict(svm_linear, newdata = testing)
#we use the predict function and pass the trained model
#and as new data we pass the testing data set
test_pred <- predict(svm_linear, newdata = testing)
#split the data into test and train data with a split of 70 (training) / 30 (testing)
set.seed(7406)
intrain <- createDataPartition(y = stroke_data$stroke, p = 0.7, list = FALSE)   #selecting our target variable "stroke"
training <- stroke_data[intrain, ]
testing <- stroke_data[-intrain, ]
#checking the dimension of the training and testing data set
dim(training)
dim(testing)
#implement the train control method with the repeated cross-validation method,
trctrl <- trainControl(method = "repeatedcv", number = 10, repeats = 3)
#implement the train function from caret
svm_linear <- train(stroke ~., data = training, method = "svmLinear",
trControl=trctrl,
prepProcess = c("center", "scale"),
tuneLenght = 10)
svm_linear
#we use the predict function and pass the trained model
#and as new data we pass the testing data set
test_pred <- predict(svm_linear, newdata = testing)
test_pred
#testing the accuracy of our model with confusionMatrix
confusionMatrix(table(test_pred, testing$stroke))
plot(svm_linear, test_pred)
plot(svm_linear, test_pred)
#define different values for c
grid <- expand.grid(C = c(0, 0.01, 0.05, 0.1, 0.25, 0.5, 0.75, 1, 1.25, 1.5, 1.75, 2.5))
#improving the performance
svm_linear_grid <- train(stroke ~., data = training, method = "svmLinear",
trControl=trctrl,
preProcess = c("center", "scale"),
tuneGrid = grid,
tuneLength = 10)
svm_linear_grid
setwd("~/switchdrive/MPM/group-work/sioux-mach-learn-project")
setwd("~/switchdrive/MPM/group-work/sioux-mach-learn-project/data")
setwd("~/switchdrive/MPM/group-work/sioux-mach-learn-project/data")
setwd("~/switchdrive/MPM/group-work/sioux-mach-learn-project/data")
packages <- c("dplyr","readxl", "curl", "ggplot2", "ggrepel", "maps", "plotly", "stringr", "tm", "wordcloud2", "tidyverse", "RColorBrewer", "ggwordcloud", "viridis", "bookdown", "utils", "leaps", "broom","GGally", "e1071", "caret", "mgcv")
install.packages("caret")
install.packages("caret")
knitr::opts_chunk$set(echo = TRUE, messages = FALSE)
packages <- c("dplyr","readxl", "curl", "ggplot2", "ggrepel", "maps", "plotly", "stringr", "tm", "wordcloud2", "tidyverse", "RColorBrewer", "ggwordcloud", "viridis", "bookdown", "utils", "leaps", "broom","GGally", "e1071", "caret", "mgcv")
package.check <- lapply(packages, FUN = function(x) {
if (!require(x, character.only = TRUE)) {
install.packages(x, dependencies = TRUE)
library(x, character.only = TRUE)
}
})
# make a df with only numeric values
numeric_var_original_scale <- stroke_data_nn %>% select(where(is.numeric) & !where(is.integer))
# read the csv agein, because the data needs to be idfferently prepared than in other model
stroke_data_nn <- read_csv('healthcare-dataset-stroke-data.csv')
# have a look at the data -> it is discovered, that bmi is not numeric, let's change that
str(stroke_data_nn)
stroke_data_nn[10] <- sapply(stroke_data_nn[10], as.numeric)
str(stroke_data_nn)
#install.packages("caret")
# make a df with only numeric values
numeric_var_original_scale <- stroke_data_nn %>% select(where(is.numeric) & !where(is.integer))
cat_var <- stroke_data_nn[,!(names(stroke_data_nn) %in% names(numeric_var_original_scale))] %>% select(-stroke)
# have a look at the structure of each df
str(numeric_var)
# make a df with only numeric values
numeric_var_original_scale <- stroke_data_nn %>% select(where(is.numeric) & !where(is.integer))
cat_var <- stroke_data_nn[,!(names(stroke_data_nn) %in% names(numeric_var_original_scale))] %>% select(-stroke)
# have a look at the structure of each df
str(numeric_var)
# have a look at the structure of each df
str(numeric_var)
head(stroke_data)
#get parameters of center scale to use in train and unseen data with caret library
library(caret)
preproc.param_comp <- numeric_var_original_scale  %>% preProcess(method = c("center", "scale"))
numeric_var <- preproc.param_comp %>% predict(numeric_var_original_scale)
predictors_var= cbind.data.frame(cat_var,numeric_var)
# make a df with only numeric values
numeric_var_original_scale <- stroke_data_nn %>% select(where(is.numeric) & !where(is.integer))
cat_var <- stroke_data_nn[,!(names(stroke_data_nn) %in% names(numeric_var_original_scale))] %>% select(-stroke)
knitr::opts_chunk$set(echo = TRUE, messages = FALSE)
packages <- c("dplyr","readxl", "curl", "ggplot2", "ggrepel", "maps", "plotly", "stringr", "tm", "wordcloud2", "tidyverse", "RColorBrewer", "ggwordcloud", "viridis", "bookdown", "utils", "leaps", "broom","GGally", "e1071", "caret", "mgcv")
package.check <- lapply(packages, FUN = function(x) {
if (!require(x, character.only = TRUE)) {
install.packages(x, dependencies = TRUE)
library(x, character.only = TRUE)
}
})
stroke_data <- read_csv('./data/healthcare-dataset-stroke-data.csv')
#stroke_data$bmi <- as.numeric(stroke_data$bmi)
#stroke_data$bmi[is.na(stroke_data$bmi)] <- mean(stroke_data$bmi, na.rm = TRUE)
stroke_data$age <- as.integer(stroke_data$age)
stroke_data$smoking_status <- as.factor(stroke_data$smoking_status)
stroke_data$work_type <- as.factor(stroke_data$work_type)
stroke_data$gender <- as.factor(stroke_data$gender)
stroke_data$ever_married <- as.factor(stroke_data$ever_married)
stroke_data$Residence_type <- as.factor(stroke_data$Residence_type)
stroke_data$stroke_num <- as.numeric(stroke_data$stroke)
stroke_data$stroke <- as.factor(stroke_data$stroke)
stroke_data
head(stroke_data)
#rename parameter residence_type to lower case for stylistic purposes
stroke_data <- stroke_data %>% rename("residence_type" = "Residence_type")
#check for the dimension of the data set
dim(stroke_data)
#check for missing values
colSums(is.na(stroke_data))
#convert bmi as number and replace missing values with the mean value of bmi
stroke_data$bmi <- as.numeric(stroke_data$bmi)
stroke_data$bmi[is.na(stroke_data$bmi)] <- mean(stroke_data$bmi, na.rm = TRUE)
#comparing gender with stroke
stroke_by_gender = table(stroke_data$gender, stroke_data$stroke)
names(dimnames(stroke_by_gender))<- c("Gender", "Stroke")
stroke_by_gender
#testing the effect of non smokers and smokers
count_by_smoke_status <- stroke_data %>%
select(smoking_status, stroke) %>%
group_by(smoking_status, stroke) %>%
summarise( N = n())
#testing the effect of work type
count_by_work_type <- stroke_data %>%
select(work_type, stroke) %>%
group_by(work_type, stroke) %>%
summarise( N = n())
# testing the effects of residence type
count_by_residence_type <- stroke_data %>%
select(residence_type, stroke) %>%
group_by(residence_type, stroke) %>%
summarise( N = n())
# testing the effects of gender
count_by_gender <- stroke_data %>%
select(gender, stroke) %>%
group_by(gender, stroke) %>%
summarise( N = n())
# testing the effects of hypertension
count_by_hypertension <- stroke_data %>%
select(hypertension, stroke) %>%
group_by(hypertension, stroke) %>%
summarise( N = n())
# testing the effects of heart disease
count_by_heart_disease <- stroke_data %>%
select(heart_disease, stroke) %>%
group_by(heart_disease, stroke) %>%
summarise( N = n())
# testing the effects of marriage status
count_by_marriage <- stroke_data %>%
select(ever_married, stroke) %>%
group_by(ever_married, stroke) %>%
summarise( N = n())
stroke_data %>%
ggplot(mapping = aes(x = bmi, y = age, color = stroke)) +
geom_point() +
geom_smooth(method = 'lm')
stroke_data %>%
filter(stroke == 1) %>%
ggplot(mapping = aes(x = bmi, y = age, color = smoking_status)) +
geom_point(method = 'lm') +
geom_smooth(method = 'lm')
stroke_data %>%
ggplot(mapping = aes(x = bmi, y = avg_glucose_level, color = stroke)) +
geom_point() +
geom_smooth(method = 'lm')
stroke_data %>%
filter(stroke == 1) %>%
ggplot(mapping = aes(x = bmi, y = avg_glucose_level, color = stroke)) +
geom_point(method = 'lm') +
geom_smooth(method = 'lm')
#Boxplot on avg. glucose_level
ggplot(stroke_data, aes(y = avg_glucose_level)) +
geom_boxplot()
#Boxplot on Age
ggplot(stroke_data, aes(y = age)) +
geom_boxplot()
stroke_by_age <- stroke_data %>%
# dplyr::filter(stroke == 1) %>%
ggplot(aes(x = stroke,
y = age)) +
geom_boxplot() +
labs(title = "Boxplot on no-Stroke (0) and Stroke (1) by Age",
x = "No-stroke (0) and Stroke (1)",
y = "Age")
color = "green"
stroke_by_age
#Boxplot on BMI
ggplot(stroke_data, aes(y = bmi)) +
geom_boxplot()
stroke_by_bmi <- stroke_data %>%
# dplyr::filter(stroke == 1) %>%
ggplot(aes(x = stroke,
y = bmi)) +
geom_boxplot(color="orange", fill="yellow", alpha=0.2) +
ggtitle("BMI by Stroke") +
xlab("Stroke") + ylab("BMI") +
theme_minimal() + theme(axis.text.x = element_text(angle = 0))
stroke_by_bmi
#boxplot by avg_glucose_level and stroke
stroke_by_avg_glucose_level <- stroke_data %>%
# dplyr::filter(stroke == 1) %>%
ggplot(aes(x = stroke,
y = avg_glucose_level)) +
geom_boxplot(color="orange", fill="yellow", alpha=0.2) +
ggtitle("Average Glucose Level by Stroke") +
xlab("Stroke") + ylab("Avg. Glucose Level") +
theme_minimal() + theme(axis.text.x = element_text(angle = 0))
stroke_by_avg_glucose_level
set.seed(7406)
n=dim(stroke_data[1])  # number of observations in dataset
n_train=0.70*n  # training set is 70%
flag = sort(sample(1:n, size=n_train, replace=FALSE))
# Use df (all data points without ID column) df_train, and df_test
# Gender, hypertension, heart disease, ever married, work type, residence type, smoking status, and stroke are all factor types.
# This should allow for the best modeling options possible for our methods.
df_train = stroke_data[flag,]
df_test = stroke_data[-flag,]
head(df_test)
head(df_train)
set.seed(7406)
n=dim(stroke_data[1])  # number of observations in dataset
n_train=0.70*n  # training set is 70%
flag = sort(sample(1:n, size=n_train, replace=FALSE))
# Use df (all data points without ID column) df_train_linear, and df_test_linear
# Gender, hypertension, heart disease, ever married, work type, residence type, smoking status, and stroke are all factor types.
# This should allow for the best modeling options possible for our methods.
df_train_linear = stroke_data[flag,]
df_test_linear = stroke_data[-flag,]
head(df_test_linear)
head(df_train_linear)
# fitting models for simple regression model
lm.stroke.test <- lm(stroke_num ~ ., data = df_train_linear)
summary(lm.stroke.test)
# fitting models for simple regression model
linear_model <- stroke_num ~ age + hypertension + heart_disease + avg_glucose_level
# fitting models for simple regression model
lm.married <- lm(age ~ ever_married, data = df_train_linear)
summary(lm.married)
plot(age ~ ever_married, data = df_train_linear)
abline(lm.married)
# fitting models for simple regression model
lm.stroke <- lm(stroke_num ~ age + hypertension + heart_disease * avg_glucose_level, data = df_train_linear)
summary(lm.stroke)
plot(stroke_num ~ age + hypertension + heart_disease + avg_glucose_level, data = df_train_linear)
abline(lm.stroke)
# Evaluating model fit using predict linear regression, example of a self-constructed confusion matrix
predicted.lm.stroke <- ifelse(predict(lm.stroke, df_test_linear, type = "response") < 0.15, yes = 0, no = 1)
head(predicted.lm.stroke)
obs.predicted.comp.lm <- data.frame(obs = df_test_linear$stroke_num, predicted = predicted.lm.stroke)
table(obs = obs.predicted.comp.lm$obs, fit = obs.predicted.comp.lm$predicted)
table(obs = obs.predicted.comp.lm$obs, fit = obs.predicted.comp.lm$predicted) %>%
prop.table() %>%
round(digits = 2)
# Evaluating model fit using predict linear regression using confusionMatrix()
predicted.lm.stroke <- ifelse(predict(lm.stroke, df_test_linear, type = "response") < 0.15, yes = 0, no = 1)
head(predicted.lm.stroke)
confusionMatrix(as.factor(predicted.lm.stroke), as.factor(df_test_linear$stroke))
# text
glm.stroke.poisson <- glm(stroke_num ~ age + heart_disease + hypertension + avg_glucose_level,
family = "poisson",
data = df_train_linear)
summary(glm.stroke.poisson)
#plot(stroke_num ~ age + heart_disease + hypertension + avg_glucose_level, data = df_train_linear)
#abline(glm.stroke_poisson)
ggplot(data = df_train_linear, aes(x = avg_glucose_level, y = stroke_num)) +
geom_jitter(width = 0, height = 0.05) +
geom_smooth(method = "glm", method.args = list(family = "poisson"))
# Evaluating model fit poisson
# fitted(glm.stroke.poisson)
fitted.glm.stroke.poisson <- ifelse(fitted(glm.stroke.poisson) < 0.2, yes = 0, no = 1)
head(fitted.glm.stroke.poisson)
obs.fitted.comp.poisson <- data.frame(obs = df_train_linear$stroke_num, fitted = fitted.glm.stroke.poisson)
table(obs = obs.fitted.comp.poisson$obs, fit = obs.fitted.comp.poisson$fitted)
table(obs = obs.fitted.comp.poisson$obs, fit = obs.fitted.comp.poisson$fitted) %>%
prop.table() %>%
round(digits = 2)
# Evaluating model fit using predict linear regression using confusionMatrix()
glm.stroke.poisson.prediction <- as.factor(ifelse(predict(glm.stroke.poisson, df_test_linear, type = "response") < 0.15, yes = 0, no = 1))
head(glm.stroke.poisson.prediction)
confusionMatrix(glm.stroke.poisson.prediction, df_test_linear$stroke)
# Include all variables to start variable selection
# Plot all of them for visual analysis
glm.stroke.binomial <- glm(stroke ~ age + gender + avg_glucose_level + residence_type + work_type + heart_disease + hypertension + ever_married + bmi + smoking_status,
family = "binomial",
data = df_train_linear)
summary(glm.stroke.binomial)
prediction.glm <- predict(glm.stroke.binomial, df_test_linear, type = "response")
# InformationValue::optimalCutoff(df_test_linear, prediction.glm)
predicted.glm.stroke.binomial <- as.factor(ifelse(predict(glm.stroke.binomial, df_test_linear, type = "response") < 0.15, yes = 0, no = 1))
head(predicted.glm.stroke.binomial)
confusionMatrix(predicted.glm.stroke.binomial, df_test_linear$stroke)
# First iteration with parameters chosen from intuitive domain knowledge and exploratory analysis of data
glm.stroke.binomial <- glm(stroke ~ age + heart_disease + hypertension + work_type + avg_glucose_level + smoking_status,
family = "binomial",
data = df_train_linear)
summary(glm.stroke.binomial)
#plot(stroke ~ age + heart_disease + hypertension + work_type + avg_glucose_level + smoking_status, data = df_train_linear)
#abline(glm.stroke.binomial)
# second iteration only keeping statistically relevant parameters from previous model
glm.stroke.binomial.2 <- glm(stroke_num ~ age + heart_disease + hypertension + avg_glucose_level,
family = "binomial",
data = df_train_linear)
summary(glm.stroke.binomial.2)
# plot(stroke ~ age + heart_disease + hypertension + avg_glucose_level, data = df_train_linear)
# abline(glm.stroke.binomial.2)
pred <- predict(glm.stroke.binomial.2)
# pred
ggplot(data = df_train_linear, aes(x = age, y = stroke_num)) +
geom_jitter(width = 0, height = 0.05) +
geom_smooth(method = "glm", method.args = list(family = "binomial"))
predicted.glm.stroke.binomial.2 <- as.factor(ifelse(predict(glm.stroke.binomial.2, df_test_linear, type = "response") < 0.2, yes = 0, no = 1))
head(predicted.glm.stroke.binomial.2)
# predicted.glm.stroke.binomial.2 <- as.factor(predicted.glm.stroke.binomial.2)
# predicted.glm.stroke.binomial.2 <- relevel(predicted.glm.stroke.binomial.2, "0")
confusionMatrix(predicted.glm.stroke.binomial.2, df_test_linear$stroke)
library(mgcv)
gam.stroke <- gam(stroke_num ~ age + heart_disease + hypertension + avg_glucose_level,
family = "binomial",
data = df_train_linear)
summary(gam.stroke)
#plot(stroke ~ smoking_status, data = df_train_linear)
#abline(gam.stroke)
predicted.gam.stroke <- as.factor(ifelse(predict( gam.stroke, df_test_linear, type = "response") < 0.2, yes = 0, no = 1))
head(predicted.gam.stroke)
confusionMatrix(predicted.gam.stroke, df_test_linear$stroke)
# read the csv agein, because the data needs to be idfferently prepared than in other model
stroke_data_nn <- read_csv('healthcare-dataset-stroke-data.csv')
# have a look at the data -> it is discovered, that bmi is not numeric, let's change that
str(stroke_data_nn)
stroke_data_nn[10] <- sapply(stroke_data_nn[10], as.numeric)
str(stroke_data_nn)
#install.packages("caret")
# make a df with only numeric values
numeric_var_original_scale <- stroke_data_nn %>% select(where(is.numeric) & !where(is.integer))
cat_var <- stroke_data_nn[,!(names(stroke_data_nn) %in% names(numeric_var_original_scale))] %>% select(-stroke)
svm_model <- svm(stroke ~. , data = df_train_svm, type = "C-classification", kernel = "linear", )
summary(svm_model)
plot(svm_model, data = df_train_svm, bmi ~ age, slice = list(avg_glucose_level = 3))
#confusion matrix for training error
svm_training_prediction <- predict(svm_model, newdata = df_train_svm)
svm_training_error <- mean(svm_training_prediction != ytrain)
confusionMatrix(svm_training_prediction, df_train_svm$stroke)
svm_training_error
confusionMatrix(svm_training_prediction,df_train_svm$stroke)
#confusion matrix for training error
svm_training_prediction <- predict(svm_model, newdata = df_test_svm)
svm_training_error <- mean(svm_training_prediction != ytrain)
confusionMatrix(svm_training_prediction, df_train_svm$stroke)
#confusion matrix for training error
svm_training_prediction <- predict(svm_model, newdata = df_test_svm)
svm_training_error <- mean(svm_training_prediction != ytrain)
confusionMatrix(svm_training_prediction, df_test_svm$stroke)
svm_training_error
confusionMatrix(svm_training_prediction,df_train_svm$stroke)
#confusion matrix for training error
svm_training_prediction <- predict(svm_model, newdata = df_test_svm)
svm_training_error <- mean(svm_training_prediction != ytrain)
confusionMatrix(svm_training_prediction, df_test_svm$stroke)
svm_training_error
#confusionMatrix(svm_training_prediction,df_train_svm$stroke)

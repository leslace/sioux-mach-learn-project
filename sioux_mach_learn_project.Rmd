---
output:
  bookdown::pdf_document2:
    includes:
      in_header: latex/preamble.tex
      before_body: latex/titlepage.tex
    pandoc_args:
    - --csl
    - references/apa.csl
  bookdown::html_document2:
    pandoc_args:
    - --csl
    - references/apa.csl
  bookdown::word_document2:
    pandoc_args:
    - --csl
    - references/apa.csl
toc-title: Table of Contents
bibliography: references/references.bib
link-citations: yes
header-includes:
- \usepackage{fancyhdr}
- \pagestyle{fancy}
- \renewcommand{\headrulewidth}{0pt}
- \fancyfoot[C]{}
- \fancyfoot[R]{\thepage}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, messages = FALSE)
packages <- c("dplyr","readxl", "curl", "ggplot2", "ggrepel", "maps", "plotly", "stringr", "tm", "wordcloud2", "tidyverse", "RColorBrewer", "ggwordcloud", "viridis", "bookdown", "utils", "leaps", "broom","GGally", "e1071", "caret", "mgcv", "imbalance", "MLmetrics", "neuralnet")
package.check <- lapply(packages, FUN = function(x) {
    if (!require(x, character.only = TRUE)) {
        install.packages(x, dependencies = TRUE)
        library(x, character.only = TRUE)
    }
})
```

\fancyhead[LR]{}
\pagenumbering{roman}

\newpage
\cleardoublepage
\pagenumbering{arabic}
\fancyhead[L]{Applied Machine Learning and Predictive Modelling: Stroke Prediction}
\fancyhead[R]{MPM02}


# Introduction
Use case:
We are a smart watch manufacturer working on a new feature for stroke prevention. According to the World Health Organization (WHO) stroke is the 2nd leading cause of death globally, responsible for approximately 11% of total deaths. Therefore, our aim is to prevent stroke in the future with the help of existing data and machine learning models. 

In the following project we are going to analyze survey data that we plan to ask our users, complementing it with HR (Heart Rate) and CGM (Continuous Glucose Monitoring) data that our product already measures. We hope that our feature can prevent serious health issues and motivate our users to adopt healthier lifestyles.

For our analysis we worked with a Stroke Prediction Data set from
[@https://www.kaggle.com/datasets/fedesoriano/strokeprediction-dataset].

In the following document, different calculation and models  are used. The different models are intended to reflect both the teaching content from the course and the knowledge that the authors have gained during the learning process itself. 

For the following work, different R packages have been used. The respective packages and their installation can be found in the original R Markdown file or the README. 

# Importing Data and Change Type of Parameters

The first step was to research the relevant data. The data was imported into R. For simplicity, not all of the code is included. However, all code can be found in the original R Markdown file. 


```{r echo=T, message=FALSE, warning=FALSE, results='hide'}
stroke_data <- read_csv('./data/healthcare-dataset-stroke-data.csv')
```

```{r echo=T, results='hide'}
stroke_data$age <- as.integer(stroke_data$age)
stroke_data$smoking_status <- as.factor(stroke_data$smoking_status)
stroke_data$work_type <- as.factor(stroke_data$work_type)
stroke_data$gender <- as.factor(stroke_data$gender)
stroke_data$ever_married <- as.factor(stroke_data$ever_married)
stroke_data$Residence_type <- as.factor(stroke_data$Residence_type)
stroke_data$stroke_num <- as.numeric(stroke_data$stroke)
stroke_data$stroke <- as.factor(stroke_data$stroke)

stroke_data
```

\newpage

# Data Cleaning and Exploratory Data Analysis

The data has been prepared for an easier analysis and for fitting the models and calculations.

```{r}
head(stroke_data)
```


```{r}
#rename parameter residence_type to lower case for stylistic purposes
stroke_data <- stroke_data %>% rename("residence_type" = "Residence_type")
```

```{r}
#check for the dimension of the data set
dim(stroke_data)
```

The data set contains 201 missing values in "bmi". 

```{r}
#check for missing values
colSums(is.na(stroke_data))
```

As the data set given is already rather small and the missing values also concern data from people who had a stroke, we will replace the missing values with the mean "bmi" value instead of dropping these observations. 

```{r message=FALSE, warning=FALSE}
#convert bmi as number and replace missing values with the mean value of bmi
stroke_data$bmi <- as.numeric(stroke_data$bmi)
stroke_data$bmi[is.na(stroke_data$bmi)] <- mean(stroke_data$bmi, na.rm = TRUE)
```

\newpage

## Summaries

In the following chapter, the different variables are summarized by no stroke (0) and stroke (1). Please note that not all code is included in the PDF, the full code can be found in the R Markdown file 

```{r}
#comparing gender with stroke
stroke_by_gender = table(stroke_data$gender, stroke_data$stroke)
names(dimnames(stroke_by_gender))<- c("Gender", "Stroke")
stroke_by_gender
```


```{r message=FALSE, warning=FALSE, include=FALSE}
#testing the effect of non smokers and smokers
count_by_smoke_status <- stroke_data %>% 
  select(smoking_status, stroke) %>%
  group_by(smoking_status, stroke) %>%
  summarise( N = n())

```
```{r message=FALSE, warning=FALSE, include=FALSE}
#testing the effect of work type
 count_by_work_type <- stroke_data %>% 
   select(work_type, stroke) %>% 
   group_by(work_type, stroke) %>%
   summarise( N = n())
```
```{r message=FALSE, warning=FALSE, include=FALSE}
 # testing the effects of residence type
count_by_residence_type <- stroke_data %>% 
   select(residence_type, stroke) %>% 
   group_by(residence_type, stroke) %>%
   summarise( N = n())
```
```{r message=FALSE, warning=FALSE, include=FALSE}
 # testing the effects of gender
count_by_gender <- stroke_data %>% 
   select(gender, stroke) %>% 
   group_by(gender, stroke) %>%
   summarise( N = n())

```
```{r message=FALSE, warning=FALSE, include=FALSE}
 # testing the effects of hypertension
count_by_hypertension <- stroke_data %>% 
   select(hypertension, stroke) %>% 
   group_by(hypertension, stroke) %>%
   summarise( N = n())

```
```{r message=FALSE, warning=FALSE, include=FALSE}
# testing the effects of heart disease
count_by_heart_disease <- stroke_data %>% 
   select(heart_disease, stroke) %>% 
   group_by(heart_disease, stroke) %>%
   summarise( N = n())
```
```{r message=FALSE, warning=FALSE, include=FALSE}
# testing the effects of marriage status
count_by_marriage <- stroke_data %>% 
   select(ever_married, stroke) %>% 
   group_by(ever_married, stroke) %>%
   summarise( N = n())
```


## Scatterplots

With the following scatter plots the strength of the relation between the various parameters should be visualized in more detail. These leads to a better understanding of the data. For the sake of simplicity we do not include all plots, the whole code can be found in the rmarkdown file.


The following scatter plots shows a positive correlation between stroke (1) and age as with stroke (1) and bmi. For bmi there seems to be too many outliers to see a correlation with stroke (1).

```{r message=FALSE, warning=FALSE}
stroke_data %>% 
  ggplot(mapping = aes(x = bmi, y = age, color = stroke)) +
  geom_point() +
  geom_smooth(method = 'lm')
```


```{r scatterplots, include=FALSE}
stroke_data %>% 
  filter(stroke == 1) %>%
  ggplot(mapping = aes(x = bmi, y = age, color = smoking_status)) +
  geom_point(method = 'lm') +
  geom_smooth(method = 'lm')

stroke_data %>% 
  ggplot(mapping = aes(x = bmi, y = avg_glucose_level, color = stroke)) +
  geom_point() +
  geom_smooth(method = 'lm')

stroke_data %>% 
  filter(stroke == 1) %>%
  ggplot(mapping = aes(x = bmi, y = avg_glucose_level, color = stroke)) +
  geom_point(method = 'lm') +
  geom_smooth(method = 'lm')
```

## Boxplots

With the following box plots the distribution of the data points is visualized. Again, not all box plots are included. 


```{r boxplot_gluc, include=FALSE}
#Boxplot on avg. glucose_level
ggplot(stroke_data, aes(y = avg_glucose_level)) +
  geom_boxplot()
```

```{r include=FALSE}
#Boxplot on Age 
ggplot(stroke_data, aes(y = age)) +
  geom_boxplot() 
```

The following box plot visualizes the distribution no stroke (0) and stroke (1) by age. As we can see the median age for people suffering a stroke is higher and is around 76 years old. Also the distribution, besides a few outliers, is narrower. 

```{r echo=TRUE}
stroke_by_age <- stroke_data %>%
# dplyr::filter(stroke == 1) %>%
 ggplot(aes(x = stroke,
            y = age)) +
  geom_boxplot(color="orange", fill="yellow", alpha=0.2) +
  ggtitle("Boxplot on noStroke (0) and Stroke (1) by Age") + 
  xlab("Stroke") + ylab("Age") +
  theme_minimal() + theme(axis.text.x = element_text(angle = 0)) 
stroke_by_age 
```



```{r include=FALSE}
#Boxplot on BMI
ggplot(stroke_data, aes(y = bmi)) +
geom_boxplot()
```

In the following the distribution of the bmi by no stroke (0) and stroke (1) is shown. As we can see the Median of the two variables are nearly the same around 26. For the people with no stroke (0) the distribution of the bmi is higher. There seems to be a lot of outliers with a high bmi.  

```{r}
stroke_by_bmi <- stroke_data %>%
# dplyr::filter(stroke == 1) %>%
 ggplot(aes(x = stroke,
            y = bmi)) +
  geom_boxplot(color="orange", fill="yellow", alpha=0.2) +
  ggtitle("BMI by Stroke") + 
  xlab("Stroke") + ylab("BMI") +
  theme_minimal() + theme(axis.text.x = element_text(angle = 0))
stroke_by_bmi
```


```{r include=FALSE}
#boxplot by avg_glucose_level and stroke
stroke_by_avg_glucose_level <- stroke_data %>%
# dplyr::filter(stroke == 1) %>%
 ggplot(aes(x = stroke,
            y = avg_glucose_level)) +
  geom_boxplot(color="orange", fill="yellow", alpha=0.2) +
  ggtitle("Average Glucose Level by Stroke") + 
  xlab("Stroke") + ylab("Avg. Glucose Level") +
  theme_minimal() + theme(axis.text.x = element_text(angle = 0))

stroke_by_avg_glucose_level

```


# Methodology

!! Weglassen? Hat ja jeder seine eigenen Testsplits gemacht?!

In the following chapter the different models are implemented. As different data types and structures are needed for each model, there is no general part covering the preparation of the data. Each model has its introduction to the Methodology.

```{r}
set.seed(7406)
n=dim(stroke_data[1])  # number of observations in dataset
n_train=0.70*n  # training set is 70%
flag = sort(sample(1:n, size=n_train, replace=FALSE))
```
```{r}
# Use df (all data points without ID column) df_train, and df_test
# Gender, hypertension, heart disease, ever married, work type, residence type, smoking status, and stroke are all factor types.
# This should allow for the best modeling options possible for our methods.
df_train = stroke_data[flag,]
df_test = stroke_data[-flag,]
```

```{r}
head(df_test)
head(df_train)
```


# Linear Models and their Extensions (Fabian)

## Linear Regression

```{r}
#divide dataset into training and testing data for later cross-validation
set.seed(7406)
n=dim(stroke_data[1])  # number of observations in dataset
n_train=0.70*n  # training set is 70%
flag = sort(sample(1:n, size=n_train, replace=FALSE))
```
```{r}
# Use df (all data points without ID column) df_train_linear, and df_test_linear
# Gender, hypertension, heart disease, ever married, work type, residence type, smoking status, and stroke are all factor types.
# This should allow for the best modeling options possible for our methods.
df_train_linear = stroke_data[flag,]
df_test_linear = stroke_data[-flag,]
```


Our aim is to predict strokes, and the stroke variable is always our response variable of interest in this use case. However, simple or multiple linear regression is not the tool of choice, as stroke is a binary response variable and the relationships of predictors to response variable are not linear. 
In the following we will nonetheless fit a linear regression model to our data with stroke as the response variable. However, out of all the linear methods, we will only focus on on a binomial model with family set to "binomial" in greater detail.

## Selecting Predictors for the model

Apart from testing out individual variables, we also tested a model with all predictors to find out about the significant ones, as selecting all variables may not necessarily lead to a more accurate model, and we might run into overfitting problems. Furthermore, our variable selection is also informed by our exploratory plots above, and what we intuitively believe to be accurate risk factors for a stroke.

```{r}
# fitting models for simple regression model
lm.stroke.test <- lm(stroke_num ~ ., data = df_train_linear)
lm.stroke.test <- update(lm.stroke.test, . ~ . -stroke)
summary(lm.stroke.test)
```
Out of the significant predictors above, we chose the model below for the linear model, glm, and gam. Furthermore, on the basis of subject matter research, we can assume an interaction between stroke and heart disease as well as between BMI and glucose level. This has been taken into account and is reflected in the model below.


```{r}
# fitting model for regression
linear_model <- stroke_num ~ age + hypertension * heart_disease + bmi * avg_glucose_level
```

Although ever married theoretically returns a significant result, this is most probably confounded by age, as can be seen in the plot below, the subjects that are or were married at some point tend to be significantly older than those that have never married.
The same is most likely true for the work type, with those having never worked being younger. What surprised us, though, is that smoking status did not have as big of an effect as we would have anticipated. 


```{r}
# fitting models for simple regression model
lm.stroke <- lm(stroke_num ~ age + hypertension * heart_disease + bmi * avg_glucose_level, data = df_train_linear)
summary(lm.stroke)
```


The R-squared value suggests that only around 8% of the variation is accounted for by the model, which does not seem great.
Next, we evaluate the model fit with a confusion matrix. We are aware that this would not be suitable for "regular" linear regression, but to make it more comparable to other models we have included it anyway. In the first example, we have constructed one "manually". In all further examples we will be using the caret package's confusionMatrix() function.

```{r}
# Evaluating model fit using predict linear regression, example of a self-constructed confusion matrix

predicted.lm.stroke <- ifelse(predict(lm.stroke, df_test_linear, type = "response") < 0.15, yes = 0, no = 1)
obs.predicted.comp.lm <- data.frame(obs = df_test_linear$stroke_num, predicted = predicted.lm.stroke)
table(obs = obs.predicted.comp.lm$obs, fit = obs.predicted.comp.lm$predicted)
#expressed as percentages
table(obs = obs.predicted.comp.lm$obs, fit = obs.predicted.comp.lm$predicted) %>%
  prop.table() %>%
  round(digits = 2)
```

```{r}
# Evaluating model fit using predict linear regression using confusionMatrix()

predicted.lm.stroke <- ifelse(predict(lm.stroke, df_test_linear, type = "response") < 0.15, yes = 0, no = 1)
confusionMatrix(as.factor(predicted.lm.stroke), as.factor(df_test_linear$stroke))
print("R-squared:")
cor(as.numeric(predicted.lm.stroke), df_test_linear$stroke_num)^2
```
Surprisingly, we get both an accuracy and also specificity that is higher than expected for this mis-application of the linear model. However, not surprisingly considering that we fitted a binary response variable to a linear model, the R-squared sharply drops off in cross-validation on the test data compared to the training data.

## Generalised Linear Model with family set to Poisson

Once again, for our use case, a glm with family set to Poisson is a very sub-optimal choice, as we are dealing with a binary response variable. The only variable in our dataset that could be considered count data is the age, but trying to fit a model to the age as response variable seems equally nonsensical as fitting it to our binary response variable. So, for the sake of consistency we have again chosen to do the latter in line with our use case.

```{r}
# text
glm.stroke.poisson <- glm(stroke_num ~ age + hypertension * heart_disease + bmi * avg_glucose_level,
family = "poisson",
data = df_train_linear)
summary(glm.stroke.poisson)
```

Once again, we are aware that this would not be standard procedure for the evalution of glm set to "poisson" for model evaluation. But we have set the same cutoff for classifying predicted values into stroke / no stroke as in the linear model, and created a confusion matrix.
```{r}
# Evaluating model fit using predict linear regression using confusionMatrix()

glm.stroke.poisson.prediction <- as.factor(ifelse(predict(glm.stroke.poisson, df_test_linear, type = "response") < 0.15, yes = 0, no = 1))
confusionMatrix(glm.stroke.poisson.prediction, df_test_linear$stroke)
```

Here, we still have a higher accuracy than we expected for a misused model, with specificity being even slightly higher than with the linear model.

## Generalised Linear Model with family set to Binomial
Since we are essentially dealing with a classification issue, using logistic regression in the form of a GLM with family set to "binomial" is the best method to apply out of all the models introduced so far. For this reason, we shall go into more detail here.
We start out again with all predictors available to us, and after some experimentation have decided to only model the interaction of heart disease and hypertension, as the BMI and glucose level do not seem to yield any significant interaction in the binomial model.

```{r}
# Include all variables to start variable selection

glm.stroke.binomial <- glm(stroke ~ age + gender + avg_glucose_level + bmi + residence_type + work_type + heart_disease * hypertension + ever_married + smoking_status,
family = "binomial",
data = df_train_linear)
summary(glm.stroke.binomial)
```

```{r}
predicted.glm.stroke.binomial <- as.factor(ifelse(predict(glm.stroke.binomial, df_test_linear, type = "response") < 0.15, yes = 0, no = 1))
confusionMatrix(predicted.glm.stroke.binomial, df_test_linear$stroke)
```

Mcnemar's Test P-Value indicates a highly significant result, which in this case means that one outcome is greatly more misclassified than the other, which we can also see in the matrix. However, since our app would only indicate that the user is at an elevated risk for stroke, we think that misclassifications as false positives may not be problematic and we might want to be our results to be skewed in that direction to recommend health and lifestyle improvements. For ethical and legal reasons, though, this elevated risk would have to be very carefully communicated and the caveats and limitations clearly pointed out the the end user.
In the next step, we adjust the model to only include significant interactions from above.

```{r}
# second iteration only keeping statistically relevant parameters from previous model
glm.stroke.binomial.2 <- glm(stroke ~ age + hypertension * heart_disease + avg_glucose_level,
family = "binomial",
data = df_train_linear)
summary(glm.stroke.binomial.2)
exp(coef(glm.stroke.binomial.2))
```


```{r}
predicted.glm.stroke.binomial.2 <- as.factor(ifelse(predict(glm.stroke.binomial.2, df_test_linear, type = "response") < 0.2, yes = 0, no = 1))
# predicted.glm.stroke.binomial.2 <- as.factor(predicted.glm.stroke.binomial.2)
# predicted.glm.stroke.binomial.2 <- relevel(predicted.glm.stroke.binomial.2, "0")

confusionMatrix(predicted.glm.stroke.binomial.2, df_test_linear$stroke)
```
Using the same 0.2 cutoff to discretize values as above, we receive a slightly higher accuracy with the adjusted model compared to the one including all available predictors. However, this seems to come at a cost of specificity, i.e. the new model does less misclassifications (and more homogenous ones), but recognizes less actual stroke cases. Once again, we can heavily influence this by adjusting our cutoff value for discretization, depending on the use case we would want to maximize. Or rather, a decision would have to be made about which misclassifications are acceptable from both business and ethical perspectives, which we will discuss in the conclusion.


## Generalized Additive Model

For the GAM, we also chose the "binomial" family to fit our use case best. The advantage of using a GAM is the usage of splines, which we have applied to the variables age and avg_glucose_level, and the GAM will automatically fit the spline regression.
```{r}
library(mgcv)

gam.stroke <- gam(stroke ~ s(age) + hypertension * heart_disease + s(avg_glucose_level),
family = "binomial",
data = df_train_linear)
summary(gam.stroke)
#plot(stroke ~ smoking_status, data = df_train_linear)
#abline(gam.stroke)
```

```{r}
predicted.gam.stroke <- as.factor(ifelse(predict( gam.stroke, df_test_linear, type = "response") < 0.2, yes = 0, no = 1))
cor(as.numeric(predicted.gam.stroke), df_test_linear$stroke_num)^2

confusionMatrix(predicted.gam.stroke, df_test_linear$stroke)
```
Using the GAM set to the "binomial" family only yields little improvement in accuracy over the binomial GLM at the same discretization cutoff, using the same formula. However, we do receive a slight improvement in specificity while reducing the overall misclassifications, which is something that would make us consider choosing the GAM over the GLM for our use case.


# Neural Network Yves 
Stroke Data Classification using a Neural Network. In the following model "stroke_data_nn" the Neural Network model is implemented with the neuralnet package. Our target variable is the stroke parameter which contains 0 (noStroke) and 1 (stroke) which we want to predict. The prediction was computed twice to see which factor levels result in more accuracy. The training data is used for building the model and the testing data for evaluating the model.

```{r}
# First, let's get the data and make a new variable

stroke_data_nn = stroke_data
```

```{r}
# open all the libraries needed

library (dplyr)
library (tidyverse)
library (neuralnet)
library (caret)
```

```{r}
# lets ahve a look at the data 
str(stroke_data_nn)
stroke_data_nn %>% ggplot(aes(x= bmi, y = age, color= stroke)) + geom_point()
```

```{r}
#prepare the data for training
set.seed(123)
indices <- createDataPartition(stroke_data_nn$stroke, p=.7, list = F)
```

```{r}
# make supsets
train <- stroke_data_nn %>%
  slice(indices)
test_in <- stroke_data_nn %>%
  slice(-indices)%>%
  select(-stroke)
test_truth <- stroke_data_nn%>%
  slice(-indices)%>%
  pull(stroke)
```

```{r}
# train the nural networtk with the variables (hypertension, heart disease and avg. glucose level)

stroke_data_net <- neuralnet(stroke ~ hypertension + heart_disease + avg_glucose_level , train, hidden = c(4,3))

# plot the neural network itself

#plot(stroke_data_net)
```

```{r}
# lets make the first predictions
test_results <- neuralnet::compute(stroke_data_net,test_in)
test_results$net.result
# by having a look at the test_result, it can be discovered, that only the second column is of interest
test_pred <- test_results$net.result[,2]
#setting every value lower than 0.1 to 0
test_pred <- as.factor(ifelse(test_pred < 0.1, yes = 0, no = 1))

test_pred <- factor(levels(test_truth)[test_pred],levels = levels(test_truth))


conf_matrix <- confusionMatrix(test_pred,test_truth)
conf_matrix
```
It can be discovered, that the model shows an accuracy of 90%, this can probaly improved. 
The outcome shows aswell, that the model cannot really predict a stroke.

```{r}
# lets make an improved predictions
test_results01 <- neuralnet::compute(stroke_data_net,test_in)
test_results01$net.result
# by having a look at the test_result, it can be discovered, that only the second column is of interest
test_pred01 <- test_results$net.result[,2]
#setting every value lower than 0.2 to 0
test_pred01 <- as.factor(ifelse(test_pred01 < 0.2, yes = 0, no = 1))

test_pred01 <- factor(levels(test_truth)[test_pred01],levels = levels(test_truth))


conf_matrix <- confusionMatrix(test_pred01,test_truth)
conf_matrix
```
It can be discovered, that the model shows an accuracy of 94.5%, slight improvement regarding the accuracy. 
Still the Confusion MAtrix shows, that with the Data set given, a stroke cannot really be predicted.





################################



# Support Vector Machine (Larissa)

Stroke Data Classification using a Support Vector Machine. In the following two different approaches for the Stroke Data Classification are used. On one hand to compare the different approaches and on the other hand for the learning purpose. 

In the following model "svm_linear" the SVM model is implemented with the caret package. Our target variable is the stroke parameter which contains 0 (noStroke) and 1 (stroke) which we want to predict. Here we use the approach with the kernel set to linear. The first step is to split the data set into training and testing data. The training data is used for building the model and the testing data for evaluating the model.

As we need different data types and have to remove the gender "other" data point for an optimal performance of the SVM model, we create a need data set variable in the following. 

```{r}
#define new data set
svm_stroke <- stroke_data
```


```{r}
#replace gender "Other" with "Female
svm_stroke[svm_stroke == "Other"] <- "Female"
```



```{r echo=T, results='hide'}
#set numeric stoke data type to NUll
svm_stroke$stroke_num <- NULL
head(svm_stroke)
```



```{r}
#split the data into test and train data with a split of 70 (training) / 30 (testing)
set.seed(7406)
intrain <- createDataPartition(y = svm_stroke$stroke, p = 0.7, list = FALSE)   
#selecting our target variable "stroke"
training <- svm_stroke[intrain, ]
testing <- svm_stroke[-intrain, ]

```


```{r}
#checking the dimension of the training and testing data set
dim(training)
dim(testing)
```

Implement the trainControl Method. This function will control the computation overheads which allow us to use the train function which is implemented by the caret package. In the following we use the trainControl function with the repeated crossvalidation method, with a iteration of 10 and with repeates set to 3 to compute the repeated crossvalidation 3 times.

```{r}
#implement the train control method with the repeated cross-validation method, 
trctrl <- trainControl(method = "repeatedcv", number = 10, repeats = 3)
```

In the following we pass the trainControl function to our training method. In the train function we pass our target variable stroke and set the method to linear. The following output shows our train method. As our model was trained with the C value as one we now can predict our testing data set. 

```{r message=FALSE, warning=FALSE}
#implement the train function from caret
svm_linear <- train(stroke ~., data = training, method = "svmLinear",
                    trControl=trctrl,
                    prepProcess = c("center", "scale"),
                    tuneLenght = 10)
svm_linear
```

When running our testing model we will get the prediction values with 0 (noStroke) and 1 (stroke). In the following we use the predict function and pass the trained model and as new data we pass the testing data.

```{r echo=T, results='hide'}
#test predict with testing data
test_pred <- predict(svm_linear, newdata = testing)
test_pred
```

In the next step we are going to test the accuracy of our model. We do this with the help of the confusionMatrix. The following output shows, that our model has an accuracy of 94%. Which seems to be quite good. However for the sake of completeness, we going to improve our model with customizing cost values and the crossvalidation method. 

```{r}
#testing the accuracy of our model with confusionMatrix
confusionMatrix(table(test_pred, testing$stroke))
```


```{r}
#define different values for c
grid <- expand.grid(C = c(0, 0.01, 0.05, 0.1, 0.25, 0.5, 0.75, 1, 1.25, 1.5, 1.75, 2.5))
```

The following output shows that the final value used for the model was C = 0.01.

```{r message=FALSE, warning=FALSE}
#improving the performance
svm_linear_grid <- train(stroke ~., data = training, method = "svmLinear",
                         trControl=trctrl,
                         preProcess = c("center", "scale"),
                         tuneGrid = grid,
                         tuneLength = 10)

svm_linear_grid
```


```{r echo=T, results='hide'}
test_pred_grid <- predict(svm_linear_grid, newdata = testing)
test_pred_grid 
```

With the new test_pred_grid we can achieve an Accuracy of 95% which meand we could improve our model about 1%. But the Specifictiy has been come down to 0 therefore the tuned model does not seems to be better than our original one. 

```{r}
confusionMatrix(table(test_pred_grid, testing$stroke))
```

\newpage

In the following the second approach for the SVM Model is presented. In a first step we again split the data set into training and testing data sets. In the following we set the kernel to radial to compare the two different methods. 


```{r}
#create train and test data set for SVM Model with a split 70 (training) / 30 (test) 
set.seed(7406)
n=dim(svm_stroke[1])  # number of observations in dataset
n_train=0.70*n  # training set is 70%
flag = sort(sample(1:n, size=n_train, replace=FALSE))
```

```{r}
#use all parameters without first column id
df_train_svm = svm_stroke[flag,]
df_test_svm = svm_stroke[-flag,]
```


```{r}
#define stroke train and test variables
ytrain = df_train_svm$stroke
ytest = df_test_svm$stroke
```
 
```{r}
table(df_train_svm$stroke)
```

```{r echo=T, results='hide'}
summary(df_train)
summary(df_test)
```

In the following SVM Model we set the kernel to radial and cost to 5. In the following the SVM Classification Plot is shown. It seems that Age is the bigger influence on having a stroke or not compared to the BMI. 

```{r}
#setup our svm model with kernel set to radial
svm_model_radial <- svm(stroke ~. , data = df_train_svm, type = "C-classification", kernel = "radial", cost = 5)
summary(svm_model_radial)
plot(svm_model_radial, data = df_train_svm, bmi ~ age, slice = list(avg_glucose_level = 3))
```


In the following we checked the model fit with a confusion matrix with the help of the caret package's confusionMatrix() function. The following output shows an Accuracy of 94% again. It seems that we receive the same results for the linear and the radial approach. 

The training error lies by 0.05 and the test error by 0.043.

```{r}
#confusion matrix for training error
svm_training_prediction <- predict(svm_model_radial, newdata = df_train_svm)
svm_training_error <- mean(svm_training_prediction != ytrain)
confusionMatrix(svm_training_prediction, df_train_svm$stroke)
svm_training_error
confusionMatrix(svm_training_prediction,df_train_svm$stroke)
```


```{r}
#confusion matrix for test data
svm_prediction <- predict(svm_model_radial, newdata = df_test_svm)
svm_test_error <- mean(svm_prediction != ytest)
confusionMatrix(svm_prediction,df_test_svm$stroke)
svm_test_error
```

In the following we tune the parameters for the SVM while changing C.
```{r}
#tune parameters for svm
set.seed(123)
tuned_svm <- tune(svm, stroke ~ . , data = df_train_svm, ranges = list(epsilon = seq(0, 1, 0.1), cost = 2^(2:5)))
tuned_svm
```
In the following plot we see the Performance of the SVM. The darker the colour, the lower the misclassification error. Therefore, we can say, the lower values and epsilon works best for our model. 

```{r}
plot(tuned_svm)
summary(tuned_svm)
```

Now, lets choose the best model for our Data. 

```{r}
#choose the best model based on hyperparameter 
svm_after_tuned <- tuned_svm$best.model
summary(svm_after_tuned)
```

```{r}
#plot the tuned SVM model
plot(svm_after_tuned, data = df_train_svm, bmi ~ age, slice = list(avg_glucose_level = 3))
```


Again, we checked the Accuracy of the model after tuning the hyperparameters with the help of the confusion matrix. As we can see, the model receives an accuracy of 95%. Again, we receive the same results with the radial kernel as we did with the linear kernel.

```{r}
# Confusion matrix after tuning hyperparameters
svm_prediction_tuned <- predict(svm_after_tuned, newdata = df_test_svm)
svm_tuned_test_error <- mean(svm_prediction_tuned != ytest)
confusionMatrix(svm_prediction_tuned,df_test_svm$stroke)
svm_tuned_test_error
```

```{r}
#compare the tuned prediction with the test data 
confusionMatrix(svm_prediction_tuned,df_test_svm$stroke)
```


# Conclusion
All our approaches lead to interesting outcomes. The accuracy of the models(SVM,NN,GAM) range between 94% - 95%, means the predictions calculated, compared to the test data was quiet accurate. Nevertheless, when we have a closer look at the all the confusion matrix of each model, it is clearly visible, that these models cannot really predict a stroke or which factor could increase the risk of having a stroke. There are two factors that contribute to leading us to the conclusion. On the one hand, the data itself may not be comprehensive enough to precisely predict a stroke. On the other hand, the dataset is very imbalanced, skewing heavily towards non-stroke outcomes. Perhaps further analysis would have to be done with a more balanced dataset, or we would have to reduce our dataset to make it more balanced. With the very limited stroke outcomes in this dataset, however, a fully adjusted dataset for balance would have resulted in too small of a sample in our case with the data at hand.

For our use case as a smartwatch manufacturer, based on the data analyzed, we would not market stroke prediction to our users, but rather suggest healthy lifestyle adjustments based on our predictions, informing users that their current values indicate a slightly elevated risk of stroke in the long term. 
After evaluating all these different ML methods and models, as a business, we would implement the neural network approach, as it has given us slightly higher accuracy than the other models.

\newpage

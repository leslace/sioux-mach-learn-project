---
output:
  bookdown::pdf_document2:
    includes:
      in_header: latex/preamble.tex
      before_body: latex/titlepage.tex
    pandoc_args:
    - --csl
    - references/apa.csl
  bookdown::html_document2:
    pandoc_args:
    - --csl
    - references/apa.csl
  bookdown::word_document2:
    pandoc_args:
    - --csl
    - references/apa.csl
toc-title: Table of Contents
bibliography: references/references.bib
link-citations: yes
header-includes:
- \usepackage{fancyhdr}
- \pagestyle{fancy}
- \renewcommand{\headrulewidth}{0pt}
- \fancyfoot[C]{}
- \fancyfoot[R]{\thepage}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, messages = FALSE)
packages <- c("dplyr","readxl", "curl", "ggplot2", "ggrepel", "maps", "plotly", "stringr", "tm", "wordcloud2", "tidyverse", "RColorBrewer", "ggwordcloud", "viridis", "bookdown", "utils", "leaps", "broom","GGally", "e1071", "caret", "mgcv")
package.check <- lapply(packages, FUN = function(x) {
    if (!require(x, character.only = TRUE)) {
        install.packages(x, dependencies = TRUE)
        library(x, character.only = TRUE)
    }
})
```

\fancyhead[LR]{}
\pagenumbering{roman}

\newpage
\cleardoublepage
\pagenumbering{arabic}
\fancyhead[L]{Applied Machine Learning and Predictive Modelling: Stroke Data}
\fancyhead[R]{MPM02}


# Introduction
Use case:
We are a smart watch manufacturer working on a new feature for stroke prevention. According to the World Health Organization (WHO) stroke is the 2nd leading cause of death globally, responsible for approximately 11% of total deaths.
We are going to analyze survey data that we plan to ask our users, complementing it with HR (Heart Rate) and CGM (Continuous Glucose Monitoring) data that our product already measures.
We hope that our feature can prevent serious health issues and motivate our users to adopt healthier lifestyles.

We worked with a Stroke Prediction Data set from [@https://www.kaggle.com/datasets/fedesoriano/stroke-prediction-dataset].

In the following document, different calculation and models  are used. The different models are intended to reflect both the teaching content from the course and the knowledge that the authors have gained during the learning process itself. 

!! Hier evtl auch Schwierigkeit von Linear Models beschreiben??

# Importing Data

The first step was to research the relevant data. The data was imported into R. For simplicity, not all of the code is included. However, all code can be found in the original R Markdown file. 

````{r raw_data_view}
stroke_data <- read_csv('./data/healthcare-dataset-stroke-data.csv')

#stroke_data$bmi <- as.numeric(stroke_data$bmi)
#stroke_data$bmi[is.na(stroke_data$bmi)] <- mean(stroke_data$bmi, na.rm = TRUE)
stroke_data$age <- as.integer(stroke_data$age)
stroke_data$smoking_status <- as.factor(stroke_data$smoking_status)
stroke_data$work_type <- as.factor(stroke_data$work_type)
stroke_data$gender <- as.factor(stroke_data$gender)
stroke_data$ever_married <- as.factor(stroke_data$ever_married)
stroke_data$Residence_type <- as.factor(stroke_data$Residence_type)
stroke_data$stroke_num <- as.numeric(stroke_data$stroke)
stroke_data$stroke <- as.factor(stroke_data$stroke)


stroke_data

````

# Data Cleaning

The data has been prepared for an easier analysis and for fitting the models and calculations.

```{r}
head(stroke_data)
```


```{r}
#rename parameter residence_type to lower case for stylistic purposes
stroke_data <- stroke_data %>% rename("residence_type" = "Residence_type")
```

```{r}
#check for the dimension of the data set
dim(stroke_data)
```

The data set contains 201 missing values in "bmi". 

```{r}
#check for missing values
colSums(is.na(stroke_data))
```

As the data set given is already quite small, we will replace the missing values with the mean "bmi" value. 

```{r}
#convert bmi as number and replace missing values with the mean value of bmi
stroke_data$bmi <- as.numeric(stroke_data$bmi)
stroke_data$bmi[is.na(stroke_data$bmi)] <- mean(stroke_data$bmi, na.rm = TRUE)
```

!!! Ich weiss nicht ob wir hier genauer testen sollen ob das sinnvoll ist, da Matteo im R-Bootcamp recht klar gesagt hat, das der Umgang mit missing values immer sehr kritisch ist und man daher die daten besser lÃ¶scht als ersetzt. 



## Summaries


```{r}
#comparing gender with stroke
stroke_by_gender = table(stroke_data$gender, stroke_data$stroke)
names(dimnames(stroke_by_gender))<- c("Gender", "Stroke")
stroke_by_gender
```

```{r}
#testing the effect of non smokers and smokers
count_by_smoke_status <- stroke_data %>% 
  select(smoking_status, stroke) %>%
  group_by(smoking_status, stroke) %>%
  summarise( N = n())

```
```{r}
#testing the effect of work type
 count_by_work_type <- stroke_data %>% 
   select(work_type, stroke) %>% 
   group_by(work_type, stroke) %>%
   summarise( N = n())
```
```{r}
 # testing the effects of residence type
count_by_residence_type <- stroke_data %>% 
   select(residence_type, stroke) %>% 
   group_by(residence_type, stroke) %>%
   summarise( N = n())
```
```{r}
 # testing the effects of gender
count_by_gender <- stroke_data %>% 
   select(gender, stroke) %>% 
   group_by(gender, stroke) %>%
   summarise( N = n())

```
```{r}
 # testing the effects of hypertension
count_by_hypertension <- stroke_data %>% 
   select(hypertension, stroke) %>% 
   group_by(hypertension, stroke) %>%
   summarise( N = n())

```
```{r}
# testing the effects of heart disease
count_by_heart_disease <- stroke_data %>% 
   select(heart_disease, stroke) %>% 
   group_by(heart_disease, stroke) %>%
   summarise( N = n())
```
```{r}
# testing the effects of marriage status
count_by_marriage <- stroke_data %>% 
   select(ever_married, stroke) %>% 
   group_by(ever_married, stroke) %>%
   summarise( N = n())
```


## Scatterplots


```{r scatterplots}
stroke_data %>% 
  ggplot(mapping = aes(x = bmi, y = age, color = stroke)) +
  geom_point() +
  geom_smooth(method = 'lm')

stroke_data %>% 
  filter(stroke == 1) %>%
  ggplot(mapping = aes(x = bmi, y = age, color = smoking_status)) +
  geom_point(method = 'lm') +
  geom_smooth(method = 'lm')

stroke_data %>% 
  ggplot(mapping = aes(x = bmi, y = avg_glucose_level, color = stroke)) +
  geom_point() +
  geom_smooth(method = 'lm')

stroke_data %>% 
  filter(stroke == 1) %>%
  ggplot(mapping = aes(x = bmi, y = avg_glucose_level, color = stroke)) +
  geom_point(method = 'lm') +
  geom_smooth(method = 'lm')
```

## Boxplots

```{r boxplot_gluc}
#Boxplot on avg. glucose_level
ggplot(stroke_data, aes(y = avg_glucose_level)) +
  geom_boxplot()
```

```{r}
#Boxplot on Age 
ggplot(stroke_data, aes(y = age)) +
  geom_boxplot() 
```

```{r}
stroke_by_age <- stroke_data %>%
# dplyr::filter(stroke == 1) %>%
 ggplot(aes(x = stroke,
            y = age)) +
  geom_boxplot() +
  labs(title = "Boxplot on no-Stroke (0) and Stroke (1) by Age",
       x = "No-stroke (0) and Stroke (1)",
       y = "Age")
       color = "green"

stroke_by_age 
```



```{r}
#Boxplot on BMI
ggplot(stroke_data, aes(y = bmi)) +
geom_boxplot()
```

```{r}
stroke_by_bmi <- stroke_data %>%
# dplyr::filter(stroke == 1) %>%
 ggplot(aes(x = stroke,
            y = bmi)) +
  geom_boxplot(color="orange", fill="yellow", alpha=0.2) +
  ggtitle("BMI by Stroke") + 
  xlab("Stroke") + ylab("BMI") +
  theme_minimal() + theme(axis.text.x = element_text(angle = 0))
stroke_by_bmi
```


```{r}
#boxplot by avg_glucose_level and stroke
stroke_by_avg_glucose_level <- stroke_data %>%
# dplyr::filter(stroke == 1) %>%
 ggplot(aes(x = stroke,
            y = avg_glucose_level)) +
  geom_boxplot(color="orange", fill="yellow", alpha=0.2) +
  ggtitle("Average Glucose Level by Stroke") + 
  xlab("Stroke") + ylab("Avg. Glucose Level") +
  theme_minimal() + theme(axis.text.x = element_text(angle = 0))

stroke_by_avg_glucose_level

```


# Methodology

```{r}
set.seed(7406)
n=dim(stroke_data[1])  # number of observations in dataset
n_train=0.70*n  # training set is 70%
flag = sort(sample(1:n, size=n_train, replace=FALSE))
```
```{r}
# Use df (all data points without ID column) df_train, and df_test
# Gender, hypertension, heart disease, ever married, work type, residence type, smoking status, and stroke are all factor types.
# This should allow for the best modeling options possible for our methods.
df_train = stroke_data[flag,]
df_test = stroke_data[-flag,]
```

```{r}
head(df_test)
head(df_train)
```


# Linear Models

```{r}
set.seed(7406)
n=dim(stroke_data[1])  # number of observations in dataset
n_train=0.70*n  # training set is 70%
flag = sort(sample(1:n, size=n_train, replace=FALSE))
```
```{r}
# Use df (all data points without ID column) df_train_linear, and df_test_linear
# Gender, hypertension, heart disease, ever married, work type, residence type, smoking status, and stroke are all factor types.
# This should allow for the best modeling options possible for our methods.
df_train_linear = stroke_data[flag,]
df_test_linear = stroke_data[-flag,]
```

```{r}
head(df_test_linear)
head(df_train_linear)
```

Our aim is to predict strokes, and the stroke variable is always our response variable of interest. However, simple or multiple linear regression is not the tool of choice, as stroke is a binary / categorical response variable. 
Thus in the following we will nonetheless fit a linear regression model to our data with stroke as the response variable (and another example to show that we've understood the concept).
We know of the following limitations for fitting a linear regression model to a binary response variable:
- 
- 
- 

```{r}
# qplot -> age/glucose level per stroke
library(ggplot2)
 qplot(y = age, x = avg_glucose_level,
        data = df_train_linear,
        facets = ~ stroke)
```
```{r}
# fitting models for simple regression model
lm.stroke <- lm(stroke_num ~ gender + age + hypertension + heart_disease + ever_married + work_type + residence_type + avg_glucose_level + bmi + smoking_status , data = df_train_linear)
summary(lm.stroke)
 
plot(stroke_num ~ gender + age + hypertension + heart_disease + ever_married + work_type + residence_type + avg_glucose_level + bmi + smoking_status, data = df_train_linear)
abline(lm.stroke)
```

```{r}
# fitting models for simple regression model
lm.stroke.smoking <- lm(stroke_num ~ smoking_status , data = df_train_linear)
summary(lm.stroke.smoking)
 
plot(stroke_num ~ smoking_status, data = df_train_linear)
abline(lm.stroke.smoking)
```

```{r}
# fitting models for simple regression model
lm.stroke.glucose <- lm(stroke_num ~ avg_glucose_level , data = df_train_linear)
summary(lm.stroke.smoking)
 
plot(stroke_num ~ avg_glucose_level, data = df_train_linear)
abline(lm.stroke.glucose)
```
```{r}
# fitting models for simple regression model
pred_var  <- 

lm.stroke.glucose <- lm(stroke_num ~ age + gender , data = df_train_linear)
summary(lm.stroke.smoking)
 
plot(stroke_num ~ age + gender, data = df_train_linear)
abline(lm.stroke.glucose)
```


```{r}
# fitting models for simple regression model
 lm.stroke.bmi <- lm(bmi ~ avg_glucose_level, data = df_train_linear)
 summary(lm.stroke.bmi)
 plot(bmi ~ avg_glucose_level, data = df_train_linear)
 abline(lm.stroke.bmi)
```



# Generalised Linear Model with family set to Poisson

```{r}
# text
glm.stroke.poisson <- glm(stroke_num ~ age + heart_disease + hypertension + avg_glucose_level,
family = "poisson",
data = df_train_linear)
summary(glm.stroke.poisson)
#plot(stroke_num ~ age + heart_disease + hypertension + avg_glucose_level, data = df_train_linear)
#abline(glm.stroke_poisson)
ggplot(data = df_train_linear, aes(x = avg_glucose_level, y = stroke_num)) + 
  geom_jitter(width = 0, height = 0.05) +
  geom_smooth(method = "glm", method.args = list(family = "poisson"))

```

```{r}
# Evaluating model fit poisson

# fitted(glm.stroke.poisson)
fitted.glm.stroke.poisson <- ifelse(fitted(glm.stroke.poisson) < 0.08, yes = 0, no = 1)
head(fitted.glm.stroke.poisson)

obs.fitted.comp.poisson <- data.frame(obs = df_train_linear$stroke_num, fitted = fitted.glm.stroke.poisson)

table(obs = obs.fitted.comp.poisson$obs, fit = obs.fitted.comp.poisson$fitted)

table(obs = obs.fitted.comp.poisson$obs, fit = obs.fitted.comp.poisson$fitted) %>%
  prop.table() %>%
  round(digits = 2)
```



# Generalised Linear Model with family set to Binomial
Since we are essentially dealing with a classification issue, using logistic regression in the form of a GLM with family set to "binomial" is the best method to apply out of all the models introduced so far. For this reason, we shall go into more detail here.

```{r}
# Include all variables to start variable selection
# Plot all of them for visual analysis
glm.stroke.binomial <- glm(stroke ~ .,
family = "binomial",
data = df_train_linear)
summary(glm.stroke.binomial)
plot(stroke ~ ., data = df_train_linear)
abline(glm.stroke.binomial)
```

```{r}
# First iteration with parameters chosen from intuitive domain knowledge and exploratory analysis of data
glm.stroke.binomial <- glm(stroke ~ age + heart_disease + hypertension + work_type + avg_glucose_level + smoking_status,
family = "binomial",
data = df_train_linear)
summary(glm.stroke.binomial)
plot(stroke ~ age + heart_disease + hypertension + work_type + avg_glucose_level + smoking_status, data = df_train_linear)
abline(glm.stroke.binomial)
```
```{r}
# second iteration only keeping statistically relevant parameters from previous model
glm.stroke.binomial.2 <- glm(stroke_num ~ age + heart_disease + hypertension + avg_glucose_level,
family = "binomial",
data = df_train_linear)
summary(glm.stroke.binomial.2)
 plot(stroke ~ age + heart_disease + hypertension + avg_glucose_level, data = df_train_linear)
 abline(glm.stroke.binomial.2)

pred <- predict(glm.stroke.binomial.2)
# pred

ggplot(data = df_train_linear, aes(x = age, y = stroke_num)) + 
  geom_jitter(width = 0, height = 0.05) +
  geom_smooth(method = "glm", method.args = list(family = "binomial"))

```
```{r}
# Evaluating model fit using fitted()

# fitted(glm.stroke.binomial.2)
fitted.glm.stroke.binomial.2 <- ifelse(fitted(glm.stroke.binomial.2) < 0.2, yes = 0, no = 1)
head(fitted.glm.stroke.binomial.2)

obs.fitted.comp <- data.frame(obs = df_train_linear$stroke_num, fitted = fitted.glm.stroke.binomial.2)

table(obs = obs.fitted.comp$obs, fit = obs.fitted.comp$fitted)

table(obs = obs.fitted.comp$obs, fit = obs.fitted.comp$fitted) %>%
  prop.table() %>%
  round(digits = 2)
```

```{r}
# Evaluating model fit using predict

# predict(glm.stroke.binomial.2, df_test_linear, type = "response")

# predicted(glm.stroke.binomial.2)
predicted.glm.stroke.binomial.2 <- ifelse(predict(glm.stroke.binomial.2, df_test_linear, type = "response") < 0.2, yes = 0, no = 1)
head(predicted.glm.stroke.binomial.2)

obs.predicted.comp <- data.frame(obs = df_test_linear$stroke_num, predicted = predicted.glm.stroke.binomial.2)

table(obs = obs.predicted.comp$obs, fit = obs.predicted.comp$predicted)

table(obs = obs.predicted.comp$obs, fit = obs.predicted.comp$predicted) %>%
  prop.table() %>%
  round(digits = 2)
```






# Generalised Additive Model
```{r}
library(mgcv)

gam.stroke <- gam(stroke ~ smoking_status + s(bmi),
family = "binomial",
data = df_train_linear)
summary(gam.stroke)
plot(stroke ~ smoking_status, data = df_train_linear)
abline(gam.stroke)

```



# Neural Network Yves 

# Support Vector Machine (Larissa)

Stroke Data Classification using a Support Vector Machine.

```{r}
#create train and test data set for SVM Model with a split 70 (training) / 30 (test) 
set.seed(7406)
n=dim(stroke_data[1])  # number of observations in dataset
n_train=0.70*n  # training set is 70%
flag = sort(sample(1:n, size=n_train, replace=FALSE))
```

```{r}
#use all parameters without first column id
df_train_svm = stroke_data[flag,]
df_test_svm = stroke_data[-flag,]
```



```{r}
ytrain = df_train_svm$stroke
ytest = df_test_svm$stroke
```
 
```{r}
table(df_train_svm$stroke)
```

```{r}
svm_model <- svm(stroke ~. , data = df_train_svm, type = "C-classification", kernel = "radial")
summary(svm_model)
plot(svm_model, data = df_train_svm, bmi ~ age, slice = list(avg_glucose_level = 3))
```


```{r}
#confusion matrix for training error
svm_training_prediction <- predict(svm_model, newdata = df_train_svm)
svm_training_error <- mean(svm_training_prediction != ytrain)
confusionMatrix(svm_training_prediction, df_train_svm$stroke)
svm_training_error
confusionMatrix(svm_training_prediction,df_train_svm$stroke)
```


```{r}
#confusion matrix for test data
svm_prediction <- predict(svm_model, newdata = df_test_svm)
svm_test_error <- mean(svm_prediction != ytest)
confusionMatrix(svm_prediction,df_test_svm$stroke)
svm_test_error
```

```{r}
#tune parameters for svm
set.seed(123)
tuned_svm <- tune(svm, stroke ~ . , data = df_train_svm, ranges = list(epsilon = seq(0, 1, 0.1), cost = 2^(2:5)))
tuned_svm
```

```{r}
plot(tuned_svm)
summary(tuned_svm)
```

```{r}
svm_after_tuned <- tuned_svm$best.model
summary(svm_after_tuned)
```

```{r}
plot(svm_after_tuned, data = df_train_svm, bmi ~ age, slice = list(avg_glucose_level = 3))
```

```{r}
# Confusion matrix after tuning hyperparameters
svm_prediction_tuned <- predict(svm_after_tuned, newdata = df_test_svm)
svm_tuned_test_error <- mean(svm_prediction_tuned != ytest)
confusionMatrix(svm_prediction_tuned,df_test_svm$stroke)
svm_tuned_test_error
```

```{r}
confusionMatrix(svm_prediction_tuned,df_test_svm$stroke)
```

# Compare Models

# OPTIONAL solve an optimisation problem

# Conclusion

\newpage
